// Simple server to serve static files and proxy API requests
import express from 'express';
import fetch from 'node-fetch';
import path from 'path';
import { fileURLToPath } from 'url';
import dotenv from 'dotenv';

dotenv.config();

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

const app = express();
const PORT = process.env.PORT || 3000;

// Enable CORS
app.use((req, res, next) => {
    res.header('Access-Control-Allow-Origin', '*');
    res.header('Access-Control-Allow-Methods', 'GET, POST, PUT, DELETE, OPTIONS');
    res.header('Access-Control-Allow-Headers', 'Content-Type');
    if (req.method === 'OPTIONS') {
        return res.sendStatus(200);
    }
    next();
});

// Parse JSON bodies
app.use(express.json());

// Serve static files from current directory
app.use(express.static(__dirname));

// API endpoint for chat
app.post('/api/chat', async (req, res) => {
    try {
        const OpenAI = (await import('openai')).default;
        const openai = new OpenAI({
            apiKey: process.env.OPENAI_API_KEY
        });

        const { messages, temperature = 0.7, max_tokens = 150, response_format } = req.body;

        if (!messages) {
            return res.status(400).json({ error: 'messages array is required' });
        }

        console.log('[AI] Chat request:', messages[messages.length - 1].content.substring(0, 50) + '...');

        const completion = await openai.chat.completions.create({
            model: 'gpt-4o-mini',
            messages: messages,
            temperature: temperature,
            max_tokens: max_tokens,
            response_format: response_format
        });

        const content = completion.choices[0].message.content;

        // Calculate cost
        const usage = completion.usage;
        const inputCost = (usage.prompt_tokens / 1_000_000) * 0.15;
        const outputCost = (usage.completion_tokens / 1_000_000) * 0.60;

        res.json({
            success: true,
            content: content,
            tokensUsed: usage.total_tokens,
            cost: {
                inputTokens: usage.prompt_tokens,
                outputTokens: usage.completion_tokens,
                totalTokens: usage.total_tokens,
                estimatedCostUSD: (inputCost + outputCost).toFixed(6)
            }
        });

    } catch (error) {
        console.error('[AI] Chat Error:', error);
        res.status(500).json({
            error: 'Chat request failed',
            message: error.message
        });
    }
});

// API endpoint for analyzing teacher messages
app.post('/api/analyze-message', async (req, res) => {
    try {
        const OpenAI = (await import('openai')).default;
        const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

        const { teacherMessage, studentType, scenarioContext, conversationHistory } = req.body;

        const messages = [
            {
                role: 'system',
                content: `Ð¢Ñ‹ ÑÐºÑÐ¿ÐµÑ€Ñ‚-Ð¿ÐµÐ´Ð°Ð³Ð¾Ð³. ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ ÑƒÑ‡Ð¸Ñ‚ÐµÐ»Ñ Ð¸ Ð´Ð°Ð²Ð°Ð¹ ÐºÑ€Ð°Ñ‚ÐºÑƒÑŽ Ð¾Ð±Ñ€Ð°Ñ‚Ð½ÑƒÑŽ ÑÐ²ÑÐ·ÑŒ.`
            },
            {
                role: 'user',
                content: `ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ Ñ„Ñ€Ð°Ð·Ñƒ ÑƒÑ‡Ð¸Ñ‚ÐµÐ»Ñ: "${teacherMessage}"

ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚: ÑƒÑ€Ð¾Ðº Ñ ÑƒÑ‡ÐµÐ½Ð¸ÐºÐ¾Ð¼ Ñ‚Ð¸Ð¿Ð° "${studentType}".

Ð”Ð°Ð¹ ÐºÑ€Ð°Ñ‚ÐºÐ¸Ð¹ Ð°Ð½Ð°Ð»Ð¸Ð· Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ JSON:
{
  "effectiveness": "high|medium|low",
  "tone": "positive|neutral|negative",
  "suggestion": "ÐºÑ€Ð°Ñ‚ÐºÐ¸Ð¹ ÑÐ¾Ð²ÐµÑ‚ ÑƒÑ‡Ð¸Ñ‚ÐµÐ»ÑŽ"
}`
            }
        ];

        const completion = await openai.chat.completions.create({
            model: 'gpt-4o-mini',
            messages: messages,
            temperature: 0.7,
            max_tokens: 150,
            response_format: { type: "json_object" }
        });

        const analysis = JSON.parse(completion.choices[0].message.content);
        const usage = completion.usage;
        const cost = ((usage.prompt_tokens / 1_000_000) * 0.15 + (usage.completion_tokens / 1_000_000) * 0.60).toFixed(6);

        res.json({
            analysis: analysis,
            tokensUsed: usage.total_tokens,
            cost: { estimatedCostUSD: cost }
        });

    } catch (error) {
        console.error('[AI] Analyze Error:', error);
        res.status(500).json({ error: error.message });
    }
});

// API endpoint for session analysis
app.post('/api/session-analysis', async (req, res) => {
    try {
        const OpenAI = (await import('openai')).default;
        const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

        const { conversationHistory, scenarioId, duration } = req.body;

        const messages = [
            {
                role: 'system',
                content: `Ð¢Ñ‹ ÑÐºÑÐ¿ÐµÑ€Ñ‚-Ð¿ÐµÐ´Ð°Ð³Ð¾Ð³ Ð¸ Ð¿ÑÐ¸Ñ…Ð¾Ð»Ð¾Ð³. ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ ÑƒÑ€Ð¾Ðº ÑƒÑ‡Ð¸Ñ‚ÐµÐ»Ñ Ñ ÑƒÑ‡ÐµÐ½Ð¸ÐºÐ°Ð¼Ð¸ Ð¸ Ð´Ð°Ð¹ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð¾Ñ†ÐµÐ½ÐºÑƒ.`
            },
            {
                role: 'user',
                content: `Ð˜ÑÑ‚Ð¾Ñ€Ð¸Ñ ÑƒÑ€Ð¾ÐºÐ°:
${conversationHistory.map(m => `${m.role}: ${m.content}`).join('\n')}

Ð”Ð»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ: ${Math.round(duration / 60000)} Ð¼Ð¸Ð½ÑƒÑ‚.

ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ ÑƒÑ€Ð¾Ðº Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ð¾ Ð¸ Ð²ÐµÑ€Ð½Ð¸ JSON Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ:
{
  "overall_score": 75,
  "feedback": "ÐžÐ±Ñ‰Ð¸Ð¹ Ñ€Ð°Ð·Ð²ÐµÑ€Ð½ÑƒÑ‚Ñ‹Ð¹ ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸Ð¹ Ð¾ Ñ‚Ð¾Ð¼, ÐºÐ°Ðº Ð¿Ñ€Ð¾ÑˆÐµÐ» ÑƒÑ€Ð¾Ðº (3-4 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ)",
  "good_points": [
    "Ð§Ñ‚Ð¾ ÑƒÑ‡Ð¸Ñ‚ÐµÐ»ÑŒ ÑÐ´ÐµÐ»Ð°Ð» Ñ…Ð¾Ñ€Ð¾ÑˆÐ¾ (Ð¼Ð¸Ð½Ð¸Ð¼ÑƒÐ¼ 3 Ð¿ÑƒÐ½ÐºÑ‚Ð°)",
    "ÐšÐ°ÐºÐ¸Ðµ Ð¿ÐµÐ´Ð°Ð³Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¿Ñ€Ð¸ÐµÐ¼Ñ‹ Ð±Ñ‹Ð»Ð¸ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ñ‹",
    "ÐŸÐ¾Ð»Ð¾Ð¶Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¼Ð¾Ð¼ÐµÐ½Ñ‚Ñ‹ ÐºÐ¾Ð¼Ð¼ÑƒÐ½Ð¸ÐºÐ°Ñ†Ð¸Ð¸"
  ],
  "bad_points": [
    "Ð§Ñ‚Ð¾ Ð¼Ð¾Ð¶Ð½Ð¾ Ð±Ñ‹Ð»Ð¾ ÑÐ´ÐµÐ»Ð°Ñ‚ÑŒ Ð»ÑƒÑ‡ÑˆÐµ (Ð¼Ð¸Ð½Ð¸Ð¼ÑƒÐ¼ 2 Ð¿ÑƒÐ½ÐºÑ‚Ð°)",
    "Ð£Ð¿ÑƒÑ‰ÐµÐ½Ð½Ñ‹Ðµ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸",
    "ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ð½Ñ‹Ðµ Ð¼Ð¾Ð¼ÐµÐ½Ñ‚Ñ‹"
  ],
  "recommendations": [
    "ÐšÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð°Ñ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ñ 1 Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ",
    "ÐšÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð°Ñ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ñ 2",
    "ÐšÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð°Ñ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ñ 3"
  ],
  "skills": {
    "empathy": 75,
    "conflictResolution": 80,
    "boundaryKeeping": 70,
    "patience": 85
  },
  "skillsExplanation": {
    "empathy": "ÐŸÐ¾Ñ‡ÐµÐ¼Ñƒ Ð¸Ð¼ÐµÐ½Ð½Ð¾ ÑÑ‚Ð¾Ñ‚ Ð±Ð°Ð»Ð» Ð¿Ð¾ ÑÐ¼Ð¿Ð°Ñ‚Ð¸Ð¸? Ð§Ñ‚Ð¾ ÑƒÑ‡Ð¸Ñ‚ÐµÐ»ÑŒ Ð´ÐµÐ»Ð°Ð» Ñ…Ð¾Ñ€Ð¾ÑˆÐ¾/Ð¿Ð»Ð¾Ñ…Ð¾ Ð² Ð¿Ð»Ð°Ð½Ðµ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ ÑÐ¼Ð¾Ñ†Ð¸Ð¹ ÑƒÑ‡ÐµÐ½Ð¸ÐºÐ¾Ð²?",
    "conflictResolution": "ÐšÐ°Ðº ÑƒÑ‡Ð¸Ñ‚ÐµÐ»ÑŒ ÑÐ¿Ñ€Ð°Ð²Ð»ÑÐ»ÑÑ Ñ ÐºÐ¾Ð½Ñ„Ð»Ð¸ÐºÑ‚Ð°Ð¼Ð¸? ÐšÐ°ÐºÐ¸Ðµ Ð¼ÐµÑ‚Ð¾Ð´Ñ‹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð»?",
    "boundaryKeeping": "ÐÐ°ÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ñ‡ÐµÑ‚ÐºÐ¾ ÑƒÑ‡Ð¸Ñ‚ÐµÐ»ÑŒ ÑƒÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°Ð» Ð³Ñ€Ð°Ð½Ð¸Ñ†Ñ‹? ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹ Ð¸Ð· ÑƒÑ€Ð¾ÐºÐ°",
    "patience": "ÐŸÑ€Ð¾ÑÐ²Ð»ÑÐ» Ð»Ð¸ ÑƒÑ‡Ð¸Ñ‚ÐµÐ»ÑŒ Ñ‚ÐµÑ€Ð¿ÐµÐ½Ð¸Ðµ? ÐšÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ñ‹"
  }
}

**Ð’Ð°Ð¶Ð½Ð¾ Ð¿Ñ€Ð¾ Ð½Ð°Ð²Ñ‹ÐºÐ¸:**
- Ð­Ð¼Ð¿Ð°Ñ‚Ð¸Ñ (0-100): ÐŸÐ¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ ÑÐ¼Ð¾Ñ†Ð¸Ð¹ ÑƒÑ‡ÐµÐ½Ð¸ÐºÐ¾Ð², ÑÐ¾Ñ‡ÑƒÐ²ÑÑ‚Ð²Ð¸Ðµ, ÑÐ¼Ð¾Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð°Ñ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ°
- Ð Ð°Ð·Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ ÐºÐ¾Ð½Ñ„Ð»Ð¸ÐºÑ‚Ð¾Ð² (0-100): Ð£Ð¼ÐµÐ½Ð¸Ðµ Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ÑŒ ÐºÐ¾Ð¼Ð¿Ñ€Ð¾Ð¼Ð¸ÑÑÑ‹, Ð´ÐµÑÑÐºÐ°Ð»Ð°Ñ†Ð¸Ñ ÐºÐ¾Ð½Ñ„Ð»Ð¸ÐºÑ‚Ð¾Ð²
- Ð“Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ (0-100): Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ñ‡ÐµÑ‚ÐºÐ¸Ñ… Ð¿Ñ€Ð°Ð²Ð¸Ð», ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»ÑŒ Ð´Ð¸ÑÑ†Ð¸Ð¿Ð»Ð¸Ð½Ñ‹
- Ð¢ÐµÑ€Ð¿ÐµÐ½Ð¸Ðµ (0-100): Ð¡Ð¿Ð¾ÐºÐ¾Ð¹ÑÑ‚Ð²Ð¸Ðµ Ð² ÑÐ»Ð¾Ð¶Ð½Ñ‹Ñ… ÑÐ¸Ñ‚ÑƒÐ°Ñ†Ð¸ÑÑ…, Ð²Ñ‹Ð´ÐµÑ€Ð¶ÐºÐ°

ÐžÑ†ÐµÐ½ÐºÐ¸ Ð¾Ñ‚ 0 Ð´Ð¾ 100. Ð‘ÑƒÐ´ÑŒ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¼ Ð½Ð¾ ÑÐ¿Ñ€Ð°Ð²ÐµÐ´Ð»Ð¸Ð²Ñ‹Ð¼. Ð’ skillsExplanation Ð´Ð°Ð¹ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ñ‹ Ð¸Ð· ÑƒÑ€Ð¾ÐºÐ°.`
            }
        ];

        const completion = await openai.chat.completions.create({
            model: 'gpt-4o-mini',
            messages: messages,
            temperature: 0.7,
            max_tokens: 800,
            response_format: { type: "json_object" }
        });

        const analysis = JSON.parse(completion.choices[0].message.content);
        const usage = completion.usage;
        const cost = ((usage.prompt_tokens / 1_000_000) * 0.15 + (usage.completion_tokens / 1_000_000) * 0.60).toFixed(6);

        console.log('[AI] Session Analysis generated:', JSON.stringify(analysis, null, 2));

        res.json({
            ...analysis,  // Spread the analysis directly (contains overall_score, feedback, etc.)
            tokensUsed: usage.total_tokens,
            cost: { estimatedCostUSD: cost }
        });

    } catch (error) {
        console.error('[AI] Session Analysis Error:', error);
        res.status(500).json({ error: error.message });
    }
});

// Health check endpoint
app.get('/health', (req, res) => {
    res.json({ status: 'OK', message: 'AI Backend is running' });
});

app.listen(PORT, () => {
    console.log(`âœ… Server running at http://localhost:${PORT}`);
    console.log(`ðŸ“„ Open: http://localhost:${PORT}/simulator_v4_avatar.html`);
});
